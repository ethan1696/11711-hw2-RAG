"""Shared dataclasses and typed records for the crawler pipeline."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Set
from urllib.parse import urlparse


@dataclass(slots=True)
class CrawlConfig:
    """Configuration consumed by the crawler pipeline.

    The class is intentionally explicit so all modules depend on a single stable
    schema and do not re-declare common flags.
    """

    # Inputs / scope
    seeds: List[str]
    allowed_domains: List[str]
    allowed_path_prefixes: List[str] = field(default_factory=list)

    # Crawl budget and scheduling
    max_depth: int = 2
    max_pages: int = 500
    per_domain_cap: int = 150
    concurrency: int = 6

    # Network / reliability
    timeout_sec: float = 15.0
    retries: int = 2
    retry_backoff_sec: float = 1.0
    delay_between_requests_sec: float = 0.2
    user_agent: str = "Mozilla/5.0 (compatible; 11711-hw2-crawler/1.0)"
    respect_robots: bool = True

    # Output
    output_dir: str = "data/crawled_output"

    # Incremental behavior
    force_download: bool = False
    force_parse: bool = False

    # Optional parser / index metadata
    extract_title: bool = True
    extract_links: bool = True

    def normalized_allowed_domains(self) -> Set[str]:
        """Normalize domain allowlist.

        Args:
            self (CrawlConfig): Config object with the current ``allowed_domains`` list.

        Returns:
            Set[str]: Allowed domains normalized to lower case with leading dots removed.
        """
        return {d.lower().lstrip(".") for d in self.allowed_domains}

    def in_scope(self, url: str) -> bool:
        """Check whether a URL is within configured crawl scope.

        Args:
            self (CrawlConfig): Config object defining the crawling allowlist.
            url (str): URL to validate.

        Returns:
            bool: ``True`` if the URL host is within one of the allowed domains.
        """
        try:
            parsed = urlparse(url)
            hostname = (parsed.netloc or "").lower()
            if not hostname:
                return False
            domains = self.normalized_allowed_domains()
            return any(hostname == d or hostname.endswith("." + d) for d in domains)
        except Exception:
            return False


@dataclass(slots=True)
class FrontierItem:
    url: str
    depth: int = 0
    referrer: Optional[str] = None


@dataclass(slots=True)
class FetchResult:
    url: str
    status_code: Optional[int]
    content_type: Optional[str]
    final_url: str
    content: Optional[bytes]
    error: Optional[str] = None
    attempts: int = 0
    elapsed_ms: Optional[float] = None

    @property
    def ok(self) -> bool:
        """Whether the fetch result should be considered successful.

        Args:
            self (FetchResult): Fetch result object being checked.

        Returns:
            bool: ``True`` when status code is in the 2xx/3xx range and ``error`` is unset.
        """
        return self.error is None and self.status_code is not None and 200 <= self.status_code < 400


@dataclass(slots=True)
class ParseResult:
    url: str
    content_type: str
    title: str = ""
    text: str = ""
    out_links: List[str] = field(default_factory=list)
    parse_error: Optional[str] = None

    @property
    def ok(self) -> bool:
        """Whether parsing completed without an error.

        Args:
            self (ParseResult): Parse result object being checked.

        Returns:
            bool: ``True`` when ``parse_error`` is ``None``.
        """
        return self.parse_error is None


@dataclass(slots=True)
class DocumentRecord:
    doc_id: str
    url: str
    text: str
    content_type: str
    crawl_time: float
    source_domain: str
    title: str = ""
    referrer: Optional[str] = None
    links_count: int = 0
    token_count: Optional[int] = None
    char_count: Optional[int] = None
    meta: Dict[str, str] = field(default_factory=dict)


@dataclass(slots=True)
class UrlMeta:
    url: str
    status: str
    content_type: Optional[str]
    status_code: Optional[int] = None
    bytes_downloaded: int = 0
    error: Optional[str] = None
    discovered_at: Optional[float] = None
    fetched_at: Optional[float] = None
    parsed_at: Optional[float] = None


@dataclass(slots=True)
class CrawlErrorRecord:
    url: str
    stage: str
    error: str
    details: Optional[str] = None
    timestamp: Optional[float] = None


@dataclass(slots=True)
class CrawlStats:
    started_at: float = 0.0
    finished_at: float = 0.0
    discovered: int = 0
    fetched: int = 0
    parsed: int = 0
    stored: int = 0
    failed_fetch: int = 0
    failed_parse: int = 0
    skipped: int = 0
    per_domain_counts: Dict[str, int] = field(default_factory=dict)


@dataclass(slots=True)
class CrawlRunArtifacts:
    """Tracks filenames/paths generated by a single crawl run."""

    raw_html_dir: str
    raw_pdf_dir: str
    docs_jsonl: str
    errors_jsonl: str
    visited_urls_path: str
    url_meta_path: str
    crawl_config_path: str
    crawl_stats_path: str


__all__ = [
    "CrawlConfig",
    "FrontierItem",
    "FetchResult",
    "ParseResult",
    "DocumentRecord",
    "UrlMeta",
    "CrawlErrorRecord",
    "CrawlStats",
    "CrawlRunArtifacts",
]
